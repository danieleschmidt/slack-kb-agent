# LLM (Large Language Model) configuration for intelligent responses
# Copy to .env and uncomment/modify as needed

# Enable LLM integration for intelligent responses
LLM_ENABLED=true

# LLM Provider: "openai" or "anthropic"
LLM_PROVIDER=openai

# Model configuration
LLM_MODEL=gpt-3.5-turbo  # or gpt-4, claude-3-haiku-20240307, etc.
LLM_MAX_TOKENS=1000
LLM_TEMPERATURE=0.1  # Low temperature for factual responses

# API Keys (required for LLM to work)
OPENAI_API_KEY=your_openai_api_key_here
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Advanced settings
LLM_MAX_CONTEXT_TOKENS=3000  # Maximum tokens for context
LLM_TIMEOUT=30  # API request timeout in seconds
LLM_RETRY_ATTEMPTS=3
LLM_RETRY_DELAY=1.0

# Optional: Custom API base (for OpenAI-compatible endpoints)
# LLM_API_BASE=https://your-custom-endpoint.com/v1

# ⚠️  IMPORTANT:
# - LLM integration requires API keys from OpenAI or Anthropic
# - API usage will incur costs based on tokens consumed
# - Keep API keys secure and never commit them to version control
# - Monitor your API usage to avoid unexpected costs